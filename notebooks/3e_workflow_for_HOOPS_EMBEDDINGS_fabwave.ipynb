{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"display: flex; gap: 10px;\">\n",
    "  <img src=\"../images/HOOPS_AI.jpg\" style=\"width: 20%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training your custom HOOPS Embeddings Model\n",
    "\n",
    "> **Purpose**: This document is for **Data Scientists** who want to **Train custom HOOPS Embedding models**. \n",
    "\n",
    "## Overview\n",
    "\n",
    "The `EmbeddingFlowModel` is a specialized FlowModel implementation designed for **training** shape embeddings model from CAD data using contrastive learning.\n",
    "\n",
    "Thus, enabling data scientists to train custom HOOPS Embedding models on their own CAD datasets.\n",
    "\n",
    "### Training → Production Workflow\n",
    "\n",
    "1. **Train** a custom HOOPS Embeddings using `EmbeddingFlowModel` + `FlowTrainer` (this document)\n",
    "2. **Register** the trained model with `HOOPSEmbeddings.register_model()` \n",
    "3. **Deploy** for production use via `HOOPSEmbeddings` API (see notebook HOOPS_embeddings_cad_search_fabwave for an example)\n",
    "\n",
    "### When to Train Custom Models\n",
    "\n",
    "- Your CAD parts have unique geometric characteristics not captured by pre-trained models\n",
    "- You need domain-specific embeddings (e.g., specific industry, manufacturing process)\n",
    "- You have a large proprietary dataset to learn from\n",
    "- You want to optimize embedding dimensions for your use case\n",
    "\n",
    "**Note**: HOOPS AI's provided a pre-trained model (e.g., `ts3d_1M_dual_v1`) that can be used directly. See the [production guide](../Embeddings%20&%20Similarity/embeddings_and_retrieval_guide.md) on how to use it directly. Trained on a large dataset with nearly 1M parts from **public datasets (ABC, fabwave, etc)**. \n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Contrastive Learning**: Learns shape representations by distinguishing between similar and dissimilar CAD geometries\n",
    "- **Flexible Architecture**: Configurable embedding dimensions, projection layers, and training parameters\n",
    "- **Unsupervised Training**: No labels required per CAD file - learns from geometric structure alone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Using TEST LICENSE (expires February 8th, 2026 - 12 days remaining)\n",
      "   For production use, obtain your own license from Tech Soft 3D\n",
      "HOOPS AI version :  1.0.0-b2dev9 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hoops_ai\n",
    "import os\n",
    "\n",
    "hoops_ai.set_license(hoops_ai.use_test_license(), validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import hoops_ai\n",
    "from hoops_ai.dataset import DatasetLoader\n",
    "from hoops_ai.ml.EXPERIMENTAL import FlowTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOOPS AI version :  1.0.0-b2dev9 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we define our tasks in a separate file for multprocessing compatibility\n",
    "from scripts.cad_tasks_embeddings import EmbeddingModel, flows_outputdir, get_flow_name, gather_cad_files, encode_data_for_ml_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor Parameters\n",
    "\n",
    "### Essential Training Parameters\n",
    "\n",
    "#### `emb_dim` (int, default: 1024)\n",
    "The dimensionality of the learned embeddings. This determines the size of the vector representation for each CAD shape.\n",
    "- Higher dimensions can capture more detailed features but increase computational cost\n",
    "- Typical values: 512, 1024, 2048\n",
    "\n",
    "#### `lr` (float, default: 3e-4)\n",
    "Learning rate for the optimizer during training.\n",
    "- Controls the step size for gradient descent updates\n",
    "- May need adjustment based on batch size and dataset characteristics\n",
    "\n",
    "### Temperature Parameters\n",
    "\n",
    "These parameters control the contrastive loss function's sensitivity to similarities:\n",
    "\n",
    "#### `temp_init` (float, default: 0.05)\n",
    "Initial temperature value for the contrastive loss.\n",
    "- Lower values make the model more discriminative\n",
    "- Higher values create softer similarities\n",
    "\n",
    "#### `temp_min` (float, default: 0.01)\n",
    "Minimum allowed temperature during training.\n",
    "\n",
    "#### `temp_max` (float, default: 0.20)\n",
    "Maximum allowed temperature during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline\n",
    "\n",
    "The `EmbeddingFlowModel` training requires a preprocessing pipeline that gathers CAD files and extract the cad data needed for the training. Here's a complete example using the FlowManager decorators:\n",
    "\n",
    "**Task 1 - Extract**: Uses `@flowtask.extract` to gather CAD files from local storage using `CADFileRetriever`. Supports multiple CAD formats and parallel processing.\n",
    "\n",
    "**Task 2 - Prepare data for Embeddings Training**: Uses `@flowtask.transform` decorator which automatically initializes and provide an optimized datastorage and a parallel handling of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LuisSalazar\\Documents\\MAIN\\MLProject\\repo\\HOOPS-AI-tutorials\\packages\\cadfiles\\fabwave\n"
     ]
    }
   ],
   "source": [
    "datasources_dir = pathlib.Path.cwd().parent.joinpath(\"packages\",\"cadfiles\",\"fabwave\")\n",
    "print(datasources_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETl pipeline preparation of the data to be used as ML-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|INFO| FLOW | ######### Flow 'HOOPS Embedding Training' start #######\n",
      "|INFO| FLOW | \n",
      "Flow Execution Summary\n",
      "|INFO| FLOW | ==================================================\n",
      "|INFO| FLOW | Task 1: Gather CAD files from datasources\n",
      "|INFO| FLOW |     Inputs : cad_datasources\n",
      "|INFO| FLOW |     Outputs: cad_dataset\n",
      "|INFO| FLOW | Task 2: Extracting CAD ML-input for EmbeddingModel\n",
      "|INFO| FLOW |     Inputs : cad_dataset\n",
      "|INFO| FLOW |     Outputs: cad_files_encoded\n",
      "|INFO| FLOW | Task 3: AutoDatasetExportTask\n",
      "|INFO| FLOW |     Inputs : cad_files_encoded\n",
      "|INFO| FLOW |     Outputs: encoded_dataset, encoded_dataset_info, encoded_dataset_attribs\n",
      "|INFO| FLOW | \n",
      "Task Dependencies:\n",
      "|INFO| FLOW | Gather CAD files from datasources has no dependencies.\n",
      "|INFO| FLOW | Gather CAD files from datasources --> Extracting CAD ML-input for EmbeddingModel\n",
      "|INFO| FLOW | Extracting CAD ML-input for EmbeddingModel --> AutoDatasetExportTask\n",
      "|INFO| FLOW | ==================================================\n",
      "\n",
      "|INFO| FLOW | Executing ParallelTask 'Gather CAD files from datasources' with 1 items.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99fc7cdb6ff4a7d8f0e288b6e40950e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DATA INGESTION:   0%|                                                                            | 0/1 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|INFO| FLOW | Executing ParallelTask 'Extracting CAD ML-input for EmbeddingModel' with 4572 items.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915fdf5c4ca9458c88814e8434e1abbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DATA TRANSFORMATION:   0%|                                                                    | 0/4572 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|WARNING| FLOW | Total number of items with errors: 26 (0.57%)\n",
      "|WARNING| FLOW | Corrupted items are listed in 'C:\\Users\\LuisSalazar\\Documents\\MAIN\\MLProject\\repo\\HOOPS-AI-tutorials\\notebooks\\out\\flows\\HOOPS Embedding Training\\error_summary.json'.\n",
      "|INFO| FLOW | Executing SequentialTask 'AutoDatasetExportTask'.\n",
      "[DatasetInfo] Warning: 200 .data files have no matching .json\n",
      "[DatasetMerger] Using streaming merge into temporary directory store for large dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60aeae6be99e447893d7c1b96678040e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DATA STORING/LOADING:   0%|          | 0/4546 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|INFO| FLOW | Auto dataset export completed in 217.07 seconds\n",
      "Sequential Task end=====================\n",
      "|INFO| FLOW | Time taken: 269.76 seconds\n",
      "|INFO| FLOW | ######### Flow 'HOOPS Embedding Training' end ######\n",
      "\n",
      "======================================================================\n",
      "FLOW EXECUTION COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      "Dataset files created:\n",
      "  Main dataset: C:\\Users\\LuisSalazar\\Documents\\MAIN\\MLProject\\repo\\HOOPS-AI-tutorials\\notebooks\\out\\flows\\HOOPS Embedding Training\\HOOPS Embedding Training.dataset\n",
      "  Info dataset: C:\\Users\\LuisSalazar\\Documents\\MAIN\\MLProject\\repo\\HOOPS-AI-tutorials\\notebooks\\out\\flows\\HOOPS Embedding Training\\HOOPS Embedding Training.infoset\n",
      "  Attributes: C:\\Users\\LuisSalazar\\Documents\\MAIN\\MLProject\\repo\\HOOPS-AI-tutorials\\notebooks\\out\\flows\\HOOPS Embedding Training\\HOOPS Embedding Training.attribset\n",
      "  Flow file: C:\\Users\\LuisSalazar\\Documents\\MAIN\\MLProject\\repo\\HOOPS-AI-tutorials\\notebooks\\out/flows/HOOPS Embedding Training/HOOPS Embedding Training.flow\n",
      "\n",
      "Total processing time: 269.76 seconds\n",
      "Files processed: 4572\n"
     ]
    }
   ],
   "source": [
    "# Create and run the Data Flow\n",
    "flow_name = get_flow_name() \n",
    "\n",
    "cad_flow = hoops_ai.create_flow(\n",
    "    name=flow_name,\n",
    "    tasks=[gather_cad_files, encode_data_for_ml_training],  # Imported from cad_tasks_embdedding.py\n",
    "    max_workers=40,  \n",
    "    flows_outputdir=str(flows_outputdir),\n",
    "    ml_task=\"custom HOOPS Embeddings model Demo\",\n",
    "    auto_dataset_export=True,  # Enable automatic dataset merging\n",
    "    #debug=True,  # Changed to True to enable debugging\n",
    "    export_visualization=False\n",
    ")\n",
    "\n",
    "# Run the flow to process all files\n",
    "flow_output, output_dict, flow_file = cad_flow.process(inputs={'cad_datasources': [str(datasources_dir)]}, clean_ouput_dir=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FLOW EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset files created:\")\n",
    "print(f\"  Main dataset: {output_dict.get('flow_data', 'N/A')}\")\n",
    "print(f\"  Info dataset: {output_dict.get('flow_info', 'N/A')}\")\n",
    "print(f\"  Attributes: {output_dict.get('flow_attributes', 'N/A')}\")\n",
    "print(f\"  Flow file: {flow_file}\")\n",
    "print(f\"\\nTotal processing time: {output_dict.get('Duration [seconds]', {}).get('total', 0):.2f} seconds\")\n",
    "print(f\"Files processed: {output_dict.get('file_count', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DatasetExplorer] Default local cluster started: <Client: 'tcp://127.0.0.1:58863' processes=1 threads=16, memory=7.45 GiB>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d59567a44a24c4182e24f8c3673883d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file info:   0%|          | 0/4346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset Table of Contents ---\n",
      "\n",
      "EDGES_GROUP:\n",
      "  EDGE_CONVEXITIES_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "  EDGE_DIHEDRAL_ANGLES_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "  EDGE_INDICES_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "  EDGE_LENGTHS_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "  EDGE_TYPES_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "  EDGE_U_GRIDS_DATA: Shape: (337065, 10, 6), Dims: ('edge', 'dim_x', 'component'), Size: 20223900\n",
      "  FILE_ID_CODE_EDGES_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "\n",
      "FACES_GROUP:\n",
      "  FACE_AREAS_DATA: Shape: (130923,), Dims: ('face',), Size: 130923\n",
      "  FACE_DISCRETIZATION_DATA: Shape: (130923, 100, 7), Dims: ('face', 'sample', 'component'), Size: 91646100\n",
      "  FACE_INDICES_DATA: Shape: (130923,), Dims: ('face',), Size: 130923\n",
      "  FACE_LOOPS_DATA: Shape: (130923,), Dims: ('face',), Size: 130923\n",
      "  FACE_TYPES_DATA: Shape: (130923,), Dims: ('face',), Size: 130923\n",
      "  FILE_ID_CODE_FACES_DATA: Shape: (130923,), Dims: ('face',), Size: 130923\n",
      "\n",
      "GRAPH_GROUP:\n",
      "  EDGES_DESTINATION_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "  EDGES_SOURCE_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "  FILE_ID_CODE_GRAPH_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "  NUM_NODES_DATA: Shape: (337065,), Dims: ('edge',), Size: 337065\n",
      "==================================\n",
      "Columns in file_info:\n",
      "                                  name    id                             description subset table_name\n",
      "0     000757c971d4af379cff2bf219566f76     0  ...25a-d5c6-4a99-a7f1-efc5c122e442.stp    N/A  file_info\n",
      "1     000d2096f1b75775aee0cca3869062ac     1  ...376-4e32-4a62-8636-833a596c3a24.stp    N/A  file_info\n",
      "2     0018d103885051d4463bfbdf97642644     2  ...8a0-14fc-4bcc-94f5-7e206c7ab2eb.stp    N/A  file_info\n",
      "3     001ec9bc5e47900ff95873b20a8cf97e     3  ...s\\STEP\\sshettigarsleevewasher30.stp    N/A  file_info\n",
      "4     002b9953b2669f0dbf7e95a97d47e759     4  ...b39-9f03-490e-b962-55ec3761f456.stp    N/A  file_info\n",
      "5     0043ebf2efcc3849c5b61c45f4190369     5  ...aae-e8c6-40df-a29b-5770a2d5a73c.stp    N/A  file_info\n",
      "6     0045b381a0be8fbc8d49688a6479a076     6  ...ef6-87ef-45b6-8c9c-0414903668e3.stp    N/A  file_info\n",
      "7     005479bbb0a5df52f6d9cc2ad0f8253c     7  ...s\\STEP\\sshettigarsleevewasher44.stp    N/A  file_info\n",
      "8     0054a7cfbadf02dce98531f4dd7508a5     8  ...800-c9bd-4dd6-b425-30bf848fcd41.stp    N/A  file_info\n",
      "9     00a5e9cd26129c2757fe6a08407df0f9     9  ...dbe-97b8-46d4-b550-aa40dab589cd.stp    N/A  file_info\n",
      "...                                ...   ...                                     ...    ...        ...\n",
      "4336  ff3d884bd5a5e96e491a4ea638d583e0  4336  ...5ae-1468-46f5-bce0-c959e3cbf870.stp    N/A  file_info\n",
      "4337  ff3fcef336d538c95a1f04ccf01339f1  4337  ...02-e51d-43d8-bd4b-171c3589485b.step    N/A  file_info\n",
      "4338  ff69cdbe147e5b2b231a48c668268d3b  4338  ...c3a-a37f-416c-8ddb-e0683a1a8926.stp    N/A  file_info\n",
      "4339  ff860352bbd8c7af1b596835829288c1  4339  ...543-8b94-4dee-a8e5-fdc362a80359.stp    N/A  file_info\n",
      "4340  ff94b5f50c009fcc19e231c8dc4fef18  4340  ...lasses\\Brackets\\STEP\\bracket217.stp    N/A  file_info\n",
      "4341  ff96da3be8fa50b0c3516392e7c57770  4341  ...cdb-e812-42b6-a29f-5973b8b41042.stp    N/A  file_info\n",
      "4342  ffb399838ecd3d81f1e05edd13002887  4342  ...106-6aba-43ff-b412-4b97010388b2.stp    N/A  file_info\n",
      "4343  ffc34e21ca8c3b0aefc40057489b1db0  4343  ...30-ad62-4695-9e60-317db1586e81.step    N/A  file_info\n",
      "4344  ffd6cacefde868ff77c8057d55add378  4344  ...83d-2422-433d-88d1-0193c2c1471c.stp    N/A  file_info\n",
      "4345  ffd8d4beab2e24522adc38308def99ec  4345  ...200-0d37-4719-b72d-b58833bef79a.stp    N/A  file_info\n"
     ]
    }
   ],
   "source": [
    "from hoops_ai.dataset import DatasetExplorer\n",
    "\n",
    "explorer = DatasetExplorer(flow_output_file=str(flow_file))\n",
    "explorer.print_table_of_contents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we move towards running a training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LuisSalazar\\Documents\\MAIN\\MLProject\\repo\\HOOPS-AI-tutorials\\notebooks\\out\\flows\\HOOPS Embedding Training\n"
     ]
    }
   ],
   "source": [
    "flow_name = get_flow_name() \n",
    "flow_root_dir = flows_outputdir.joinpath(\"flows\", flow_name)\n",
    "print(flow_root_dir)\n",
    "\n",
    "myFlow_info        = str(flow_root_dir.joinpath(f\"{flow_name}.infoset\"))\n",
    "myFlow_dataset     = str(flow_root_dir.joinpath(f\"{flow_name}.dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DatasetExplorer] Default local cluster started: <Client: 'tcp://127.0.0.1:58882' processes=1 threads=16, memory=7.45 GiB>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a976605c44864e639ed5c4e37dcc5d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing file info:   0%|          | 0/4346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Successfully built file lists with 4346 files out of 4346 original file codes\n",
      "\n",
      "============================================================\n",
      "DATASET STRUCTURE OVERVIEW\n",
      "============================================================\n",
      "\n",
      "Group: edges\n",
      "------------------------------\n",
      "  edge_convexities: (337065,) (int32)\n",
      "  edge_dihedral_angles: (337065,) (float32)\n",
      "  edge_indices: (337065,) (int32)\n",
      "  edge_lengths: (337065,) (float32)\n",
      "  edge_types: (337065,) (int32)\n",
      "  edge_u_grids: (337065, 10, 6) (float32)\n",
      "  file_id_code_edges: (337065,) (int64)\n",
      "\n",
      "Group: faces\n",
      "------------------------------\n",
      "  face_areas: (130923,) (float32)\n",
      "  face_discretization: (130923, 100, 7) (float32)\n",
      "  face_indices: (130923,) (int32)\n",
      "  face_loops: (130923,) (int32)\n",
      "  face_types: (130923,) (int32)\n",
      "  file_id_code_faces: (130923,) (int64)\n",
      "\n",
      "Group: graph\n",
      "------------------------------\n",
      "  edges_destination: (337065,) (int32)\n",
      "  edges_source: (337065,) (int32)\n",
      "  file_id_code_graph: (337065,) (int64)\n",
      "  num_nodes: (337065,) (int64)\n",
      "\n",
      "============================================================\n",
      "Dataset split by face_types: Train=3472, Validation=438, Test=436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3472, 438, 436)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the already encoded dataset and perform the split\n",
    "cadflowdataset = DatasetLoader(merged_store_path = myFlow_dataset, parquet_file_path=myFlow_info)\n",
    "cadflowdataset.split(key='face_types', group=\"faces\",train=0.8, validation=0.1, test=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we define our trainer that will do the training job for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOOPS Embedding Model\n"
     ]
    }
   ],
   "source": [
    "flow_trainer = FlowTrainer(\n",
    "\n",
    "    flowmodel       = EmbeddingModel, # imported from cad_tasks_embeddings.py\n",
    "    datasetLoader   = cadflowdataset,\n",
    "    experiment_name = \"HOOPS_AI_train\",\n",
    "    result_dir      = flow_root_dir,\n",
    "    accelerator     = 'gpu',\n",
    "    devices         = [1], #[0]\n",
    "    max_epochs      = 10,\n",
    "    batch_size      = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "HOOPS Embedding Model - TRAINING STEP\n",
      "-----------------------------------------------------------------------------------\n",
      "Training batch size               : 64\n",
      "Adjusted learning rate (for batch): 0.002\n",
      "\n",
      "Train set contains                : 3472 samples (79.89%)\n",
      "Validation set contains           : 438 samples (10.08%)\n",
      "Test set contains                 : 436 samples (10.03%)\n",
      "Total samples                     : 4346\n",
      "Max Epoch                         : 10\n",
      "\n",
      "The trained model: C:\\Users\\LuisSalazar\\Documents\\MAIN\\MLProject\\repo\\HOOPS-AI-tutorials\\notebooks\\out\\flows\\HOOPS Embedding Training\\ml_output\\HOOPS_AI_train\\0127\\000830\\best.ckpt\n",
      "\n",
      "To monitor the logs, run:\n",
      "tensorboard --logdir results/HOOPS_AI_train/0127/000830\n",
      "-----------------------------------------------------------------------------------\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da38b94093040c6b1063824b595f7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model_path = flow_trainer.train()\n",
    "print(f\"Training finished. Model checkpoint saved in {trained_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_trainer.test(trained_model_path)\n",
    "print(\"Testing finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "The output of a Flow EmbeddingsModel are the embeddings. This lsit of float values are difficult to understand and represents a learnable representation of your CAD file.\n",
    "\n",
    "Here, we are going to use the FLowInference to get the value for a new file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hoops_ai.ml.EXPERIMENTAL import FlowInference\n",
    "from hoops_ai.ml.EXPERIMENTAL import EmbeddingFlowModel\n",
    "\n",
    "from hoops_ai.cadaccess import HOOPSLoader\n",
    "from hoops_ai.insights import CADViewer\n",
    "\n",
    "# Initialize CAD loader (needed for ML inference later)\n",
    "loader = HOOPSLoader()\n",
    "\n",
    "inference_model = FlowInference(cad_loader = loader, flowmodel = EmbeddingFlowModel(result_dir=flow_root_dir))\n",
    "inference_model.load_from_checkpoint(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = pathlib.Path.cwd().parent.joinpath(\"packages\")\n",
    "cad_file_test = str(test_files.joinpath(\"cadfiles\",\"gear_fabwave.step\"))\n",
    "\n",
    "ml_input = inference_model.preprocess(cad_file_test)    \n",
    "predictions = inference_model.predict_and_postprocess(ml_input)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposite of the other two ml models of this library, the inference needs to be complemented with a vector store.\n",
    "\n",
    "This tutorial ends here, we invite the reader to check out the notebook HOOPS EMBEDDINGS for CAD SEARCH to further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Your Trained Model for Production\n",
    "\n",
    "Once training is complete, register your custom model with `HOOPSEmbeddings` to use it in production - see notebook HOOPS EMBEDDINGS CAD SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Next Steps**:\n",
    "- Using your registered model for similarity search\n",
    "- Indexing embeddings in vector databases\n",
    "- Querying for similar parts in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HOOPS AI (GPU)",
   "language": "python",
   "name": "hoops_ai_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
