{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e156c5-f68d-4a91-821f-66cb021a0aa0",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 10px;\">\n",
    "  <img src=\"../packages/cadfiles/HOOPS_AI.jpg\" style=\"width: 20%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e7bd79-b8fb-4488-a615-458805b5f39d",
   "metadata": {},
   "source": [
    "# HOOPS AI: EXPERIMENTAL - Flow Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4649585-17db-49d8-a281-15d177e19aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOOPS AI version :  1.0.0-dev16 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hoops_ai\n",
    "import os\n",
    "\n",
    "hoops_ai.set_license(os.getenv(\"HOOPS_AI_LICENSE\"), validate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4378d20a-4930-42b1-9d09-5513b50e1f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flows_outputdir = pathlib.Path.cwd().joinpath(\"out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4302c-4cce-4149-acc7-b8bdecb36a8b",
   "metadata": {},
   "source": [
    "## Prepared the ml input files for ml training\n",
    "\n",
    "this will only create the graph files (.dgl) for training the GraphClassification model\n",
    "\n",
    "This is optimized for Trianing once you knw what data you need to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ed7b67b-2881-4bd0-a534-9dc327784df8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 22:27:47 | INFO | numexpr.utils | NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOOPS AI version :  1.0.0-dev16 \n",
      "\n",
      "Flow name: ETL_Fabwave_training\n"
     ]
    }
   ],
   "source": [
    "# Import task functions from external module for ProcessPoolExecutor compatibility\n",
    "from cad_tasks_fabwave import gather_fabwave_files, encode_data_for_ml_training, get_flow_name\n",
    "flow_name = get_flow_name()\n",
    "\n",
    "print(f\"Flow name: {flow_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data source\n",
    "datasources_dir = [str(pathlib.Path.cwd().parent.joinpath(\"packages\",\"cadfiles\",\"fabwave\"))]\n",
    "\n",
    "# Create and run the Data Flow\n",
    "cad_flow = hoops_ai.create_flow(\n",
    "    name=flow_name,\n",
    "    tasks=[gather_fabwave_files, encode_data_for_ml_training],\n",
    "    max_workers=4,\n",
    "    #debug=True,\n",
    "    flows_outputdir=str(flows_outputdir),\n",
    "    ml_task=\"Part Classification\",\n",
    "    storage_type = \"memory\",\n",
    "    #parallel_task_kwargs=parallel_task_kwargs,\n",
    "    export_visualization=False  # Disable visualization export\n",
    ")\n",
    "\n",
    "# Run the flow to process all files\n",
    "print(\"Starting flow execution with parallel processing...\")\n",
    "flow_output, output_dict, flow_file = cad_flow.process(inputs={'cad_datasources': datasources_dir})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2372d2c8-73ab-4abe-b6a9-c0d9c65a86f0",
   "metadata": {},
   "source": [
    "## Define the data split and training conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52779c2d-1d00-418d-a5ef-036f6500603c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGL files directory: C:\\Users\\LuisSalazar\\MAIN\\repos\\HOOPS-AI-tutorials\\notebooks\\out\\flows\\ETL_Fabwave_training\\dgl\n",
      "Number of DGL files generated: 4541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3633, 454, 454)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hoops_ai.dataset import DatasetLoader \n",
    "# read the dgl files form the flow output and inject it to the dataset loader\n",
    "#dgl files\n",
    "dgl_dir = flows_outputdir.joinpath(\"flows\",flow_name, \"dgl\")\n",
    "print(f\"DGL files directory: {dgl_dir}\")\n",
    "# get all .ml file in that folder a list\n",
    "dgl_files = [str(f) for f in dgl_dir.glob(\"*.ml\")]\n",
    "print(f\"Number of DGL files generated: {len(dgl_files)}\")\n",
    "\n",
    "\n",
    "cadflowdataset = DatasetLoader(graph_files=dgl_files, file_code_start=1)\n",
    "\n",
    "cadflowdataset.split(key=\"random\", train=0.8, validation=0.1, test=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de175d",
   "metadata": {},
   "source": [
    "## Train GraphClassification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582a38b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRAPH CLASSIFICATION - HOOPS GAT - MODEL\n"
     ]
    }
   ],
   "source": [
    "from hoops_ai.ml.EXPERIMENTAL import GraphClassification, FlowTrainer\n",
    "# Define the Graph Classification model\n",
    "\n",
    "PartClassification_HOOPS = GraphClassification(num_classes=45,\n",
    "                                               result_dir = flows_outputdir)\n",
    "\n",
    "flow_trainer_HOOPS = FlowTrainer(\n",
    "\n",
    "    flowmodel       = PartClassification_HOOPS,\n",
    "    datasetLoader   = cadflowdataset,\n",
    "    experiment_name = \"HOOPS_AI_train\",\n",
    "    result_dir      = flows_outputdir,\n",
    "    accelerator     = 'cpu',\n",
    "    devices         = 1, #[0]\n",
    "    max_epochs      = 1,\n",
    "    batch_size      = 32  # Reduced from 128 to 64 for memory efficiency\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331a253d-d5c4-4bbc-9149-0317c73346ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    -----------------------------------------------------------------------------------\n",
      "    GRAPH CLASSIFICATION - HOOPS GAT - MODEL  - PURIFICATION STEP\n",
      "    -----------------------------------------------------------------------------------\n",
      "    Purification batch size           : 1\n",
      "    Requested processes               : 1\n",
      "    Adjusted learning rate (for batch): 6.25e-05\n",
      "    \n",
      "    Train set contains                : 3633 samples\n",
      "    Validation set contains           : 454 samples\n",
      "    Test set contains                 : 454 samples\n",
      "    Total samples                     : 4541\n",
      "    -----------------------------------------------------------------------------------\n",
      "            \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f99b55b40b4257bc7925b0289f3d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Purification Progress:   0%|                                                                  | 0/4541 [00:00<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corrupted_items = flow_trainer_HOOPS.purify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5af5ff-4e9d-4346-a900-a7483b8613b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing corrupted items : 752\n"
     ]
    }
   ],
   "source": [
    "print(\"Removing corrupted items :\", len(corrupted_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199814de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "GRAPH CLASSIFICATION - HOOPS GAT - MODEL - TRAINING STEP\n",
      "-----------------------------------------------------------------------------------\n",
      "Training batch size               : 32\n",
      "Adjusted learning rate (for batch): 0.002\n",
      "\n",
      "Train set contains                : 3022 samples (81.79%)\n",
      "Validation set contains           : 338 samples (9.15%)\n",
      "Test set contains                 : 335 samples (9.07%)\n",
      "Total samples                     : 3695\n",
      "Max Epoch                         : 1\n",
      "\n",
      "The trained model: C:\\Users\\LuisSalazar\\MAIN\\repos\\HOOPS-AI-tutorials\\notebooks\\out\\ml_output\\HOOPS_AI_train\\0120\\222752\\best.ckpt\n",
      "\n",
      "To monitor the logs, run:\n",
      "tensorboard --logdir results/HOOPS_AI_train/0120/222752\n",
      "-----------------------------------------------------------------------------------\n",
      "        \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd17a50741c14692af709b9ba0fd9871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Index 3813 is out of range for graph_files (length: 3808). This might indicate a mismatch between dataset indices and actual file lists. Dataset has 3808 file codes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_HOOPS_model \u001b[38;5;241m=\u001b[39m \u001b[43mflow_trainer_HOOPS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished. Model checkpoint saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrained_HOOPS_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m flow_trainer_HOOPS\u001b[38;5;241m.\u001b[39mtest(trained_HOOPS_model)\n",
      "File \u001b[1;32mC:\\Users\\LUISSA~1\\MINICO~1\\envs\\HOOPS_~4\\lib\\SITE-P~1\\hoops_ai\\ml\\EXPERIMENTAL\\flow_trainer.py:239\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:737\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    733\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    735\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    736\u001b[0m )\n\u001b[1;32m--> 737\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1168\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1168\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1254\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1285\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:270\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(\n\u001b[0;32m    267\u001b[0m     dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    268\u001b[0m )\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:174\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[0;32m    173\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 174\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     batch_idx, batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:185\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetching_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:264\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;66;03m# consume the batch we just fetched\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:278\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fetch_next_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator: Iterator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     start_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_start()\n\u001b[1;32m--> 278\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetched \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_len:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;66;03m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:558\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    553\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m        a collections of batch data\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:570\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[1;34m(loader_iters)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    562\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \n\u001b[0;32m    564\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m        Any: a collections of batch data\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py:100\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    102\u001b[0m elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hoops_ai_cpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mC:\\Users\\LUISSA~1\\MINICO~1\\envs\\HOOPS_~4\\lib\\SITE-P~1\\hoops_ai\\dataset\\torch_adapter.py:54\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n",
      "File \u001b[1;32mC:\\Users\\LUISSA~1\\MINICO~1\\envs\\HOOPS_~4\\lib\\SITE-P~1\\hoops_ai\\dataset\\dataset_loader.py:48\u001b[0m, in \u001b[0;36mget_item\u001b[1;34m(self, i)\u001b[0m\n",
      "File \u001b[1;32mC:\\Users\\LUISSA~1\\MINICO~1\\envs\\HOOPS_~4\\lib\\SITE-P~1\\hoops_ai\\dataset\\dataset_loader.py:878\u001b[0m, in \u001b[0;36m_get_item_by_index\u001b[1;34m(self, idx)\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Index 3813 is out of range for graph_files (length: 3808). This might indicate a mismatch between dataset indices and actual file lists. Dataset has 3808 file codes."
     ]
    }
   ],
   "source": [
    "trained_HOOPS_model = flow_trainer_HOOPS.train()\n",
    "print(f\"Training finished. Model checkpoint saved in {trained_HOOPS_model}\")\n",
    "flow_trainer_HOOPS.test(trained_HOOPS_model)\n",
    "print(f\"Testing finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be885210-ad9d-4cee-99ea-f37d5d5dba70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HOOPS AI (CPU)",
   "language": "python",
   "name": "hoops_ai_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
