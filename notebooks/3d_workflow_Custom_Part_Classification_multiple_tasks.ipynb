{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b81ce2a-61f2-4a29-bd88-3b4e8c2e2e03",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 10px;\">\n",
    "  <img src=\"../images/HOOPS_AI.jpg\" style=\"width: 20%;\">\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd61f1",
   "metadata": {},
   "source": [
    "# CUSTOM Multiple Task for Part Classification using HOOPS AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161d57c9-8173-462f-80c0-32e37b82b45e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOOPS AI version :  1.0.0-b2dev8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hoops_ai\n",
    "import os\n",
    "\n",
    "hoops_ai.set_license(os.getenv(\"HOOPS_AI_LICENSE\"), validate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc36e6f-354e-4039-8919-6aefdd96cb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 15:36:48 | INFO | numexpr.utils | NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOOPS AI version :  1.0.0-b2dev8 \n",
      "\n",
      "Schema loaded from: C:\\Users\\LuisSalazar\\MAIN\\repos\\HOOPS-AI-tutorials\\notebooks\\manufacturing_schema.json\n",
      "Flow name: ETL_Multi_Y_Part_Classification\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "from cad_tasks_custom_part_classification import get_flow_name\n",
    "flow_name = get_flow_name()\n",
    "print(f\"Flow name: {flow_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1357d",
   "metadata": {},
   "source": [
    "## Schema Management\n",
    "\n",
    "The schema defines the label structure for classification tasks. It can be:\n",
    "- Built programmatically in code\n",
    "- Loaded from an exported JSON file\n",
    "\n",
    "Set `LOAD_SCHEMA_FROM_FILE = True` in [cad_tasks_custom_part_classification.py](cad_tasks_custom_part_classification.py) to load from file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ea4530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema mode: Loading from file\n",
      "Schema file path: C:\\Users\\LuisSalazar\\MAIN\\repos\\HOOPS-AI-tutorials\\notebooks\\manufacturing_schema.json\n",
      "Schema file exists: True\n",
      "\n",
      "Schema preview (first 3 keys): ['version', 'domain', 'groups']\n"
     ]
    }
   ],
   "source": [
    "# Verify schema export location\n",
    "from cad_tasks_custom_part_classification import SCHEMA_FILE_PATH, LOAD_SCHEMA_FROM_FILE\n",
    "import os\n",
    "\n",
    "print(f\"Schema mode: {'Loading from file' if LOAD_SCHEMA_FROM_FILE else 'Building in code'}\")\n",
    "print(f\"Schema file path: {SCHEMA_FILE_PATH}\")\n",
    "print(f\"Schema file exists: {os.path.exists(SCHEMA_FILE_PATH)}\")\n",
    "\n",
    "if os.path.exists(SCHEMA_FILE_PATH):\n",
    "    import json\n",
    "    with open(SCHEMA_FILE_PATH, 'r') as f:\n",
    "        schema_preview = json.load(f)\n",
    "    print(f\"\\nSchema preview (first 3 keys): {list(schema_preview.keys())[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d54f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Import the flow builder framework from the library\n",
    "import hoops_ai\n",
    "from hoops_ai.flowmanager import flowtask\n",
    "\n",
    "\n",
    "from hoops_ai.cadaccess import HOOPSLoader, HOOPSTools\n",
    "from hoops_ai.cadencoder import BrepEncoder\n",
    "from hoops_ai.dataset import DatasetExplorer\n",
    "from hoops_ai.storage import DataStorage, CADFileRetriever, LocalStorageProvider\n",
    "from hoops_ai.storage.datasetstorage.schema_builder import SchemaBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ed600",
   "metadata": {},
   "source": [
    "## Configuring Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43191ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration - Using simpler paths\n",
    "nb_dir = pathlib.Path.cwd()\n",
    "flows_outputdir = nb_dir.joinpath(\"out\")\n",
    "# Import task functions from external module for ProcessPoolExecutor compatibility\n",
    "from cad_tasks_custom_part_classification import gather_fabwave_files, encode_data_for_ml_training, custom_graph_classification, get_flow_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10c289",
   "metadata": {},
   "source": [
    "## ETL Data Pipeline\n",
    "\n",
    "Configure the data sources for the ETL pipeline. This example uses the FabWave CAD dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f16830-cf93-41b1-9c77-7fa516020117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data source\n",
    "datasources_dir = [str(nb_dir.parent.joinpath(\"packages\",\"cadfiles\",\"fabwave\"))\n",
    "    #str(nb_dir.parent.joinpath(\"packages\",\"cadfiles\",\"fabwave\", \"CAD_1_15_Classes\", \"Bolts\")),\n",
    "    #str(nb_dir.parent.joinpath(\"packages\",\"cadfiles\",\"fabwave\", \"CAD_1_15_Classes\", \"Bushing_Damping_Liners\")),\n",
    "    #str(nb_dir.parent.joinpath(\"packages\",\"cadfiles\",\"fabwave\", \"CAD25-45_TOTAL1000\", \"Sleeve Washers\"))   \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abb045",
   "metadata": {},
   "source": [
    "## Pipeline Execution\n",
    "\n",
    "This cell runs the ETL pipeline to generate ML inputs after encoding the data.\n",
    "\n",
    "**Configuration:** Set `SKIP_ETL = False` to run the full pipeline, or `SKIP_ETL = True` to load existing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac170b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration: Skip ETL Pipeline\n",
    "SKIP_ETL = True  # Set to True to skip ETL and load existing data\n",
    "\n",
    "# Path to existing flow file (only used if SKIP_ETL = True)\n",
    "EXISTING_FLOW_FILE = str(nb_dir.joinpath(\"out\", \"flows\", flow_name, f\"{flow_name}.flow\"))\n",
    "\n",
    "if SKIP_ETL:\n",
    "    print(\"â­ï¸  Skipping ETL Pipeline - Loading existing dataset\")\n",
    "    print(f\"Flow file: {EXISTING_FLOW_FILE}\")\n",
    "    if not os.path.exists(EXISTING_FLOW_FILE):\n",
    "        print(f\"âš ï¸  WARNING: Flow file not found at {EXISTING_FLOW_FILE}\")\n",
    "        print(\"Set SKIP_ETL = False to run the ETL pipeline\")\n",
    "    else:\n",
    "        flow_file = EXISTING_FLOW_FILE\n",
    "        print(\"âœ… Flow file loaded successfully\")\n",
    "else:\n",
    "    print(\"ðŸ”„ ETL Pipeline will run in the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c20597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ETL Pipeline - Only runs if SKIP_ETL = False\n",
    "if not SKIP_ETL:\n",
    "    # Create and run the Data Flow\n",
    "    flow_name = get_flow_name()\n",
    "    cad_flow = hoops_ai.create_flow(\n",
    "        name=flow_name,\n",
    "        tasks=[gather_fabwave_files, encode_data_for_ml_training],\n",
    "        max_workers=6,\n",
    "        flows_outputdir=str(flows_outputdir),\n",
    "        ml_task=\"Part Classification\",\n",
    "        auto_dataset_export=True,  # Enable automatic dataset merging\n",
    "        export_visualization=False  # Disable visualization export\n",
    "    )\n",
    "\n",
    "    # Run the flow to process all files\n",
    "    print(\"Starting flow execution with parallel processing...\")\n",
    "    flow_output, output_dict, flow_file = cad_flow.process(inputs={'cad_datasources': datasources_dir})\n",
    "\n",
    "    print(f\"  Flow file: {flow_file}\")\n",
    "    print(f\"\\nTotal processing time: {output_dict.get('Duration [seconds]', {}).get('total', 0):.2f} seconds\")\n",
    "    print(f\"Files processed: {output_dict.get('file_count', 0)}\")\n",
    "else:\n",
    "    print(\"âœ… Skipped ETL Pipeline - Using existing flow file\")\n",
    "    print(f\"Flow file: {flow_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17fd3f",
   "metadata": {},
   "source": [
    "## Data Serving : Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff45528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explorer = DatasetExplorer(flow_output_file=str(flow_file))\n",
    "explorer.print_table_of_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e0446c-25a2-47d4-8f02-5ec1f83f621f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(explorer.available_groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbeb63d-0ad1-4117-bd08-a3b510cf2409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(explorer.available_arrays('Labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1cf04-b4f1-4447-9bce-2f57848cf732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_distribution_info(dist, title=\"Distribution\"):\n",
    "    \"\"\"Helper function to print and visualize distribution data.\"\"\"\n",
    "    list_filecount = list()\n",
    "    for i, bin_files in enumerate(dist['file_id_codes_in_bins']):\n",
    "        list_filecount.append(bin_files.size)\n",
    "\n",
    "    dist['file_count'] =list_filecount\n",
    "    # Visualization with matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    \n",
    "    bin_centers = 0.5 * (dist['bin_edges'][1:] + dist['bin_edges'][:-1])\n",
    "    ax.bar(bin_centers, dist['file_count'], width=(dist['bin_edges'][1] - dist['bin_edges'][0]), \n",
    "           alpha=0.7, color='steelblue', edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Add file count annotations\n",
    "    for i, count in enumerate(dist['file_count']):\n",
    "        if count > 0:  # Only annotate non-empty bins\n",
    "            ax.text(bin_centers[i], count + 0.5, f\"{count}\", \n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{title} Histogram')\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673898e0-6b0e-4bcc-8455-bcba89d0b5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "face_dist = explorer.create_distribution(key=\"task_A\", bins=None, group=\"Labels\")\n",
    "print(f\"Material distribution created in {(time.time() - start_time):.2f} seconds\\n\")\n",
    "print_distribution_info(face_dist, title=\"Original FABWAVE Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b568ed-53c9-45f6-b7fc-5e0ee8bc11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "face_dist = explorer.create_distribution(key=\"task_C\", bins=None, group=\"Labels\")\n",
    "print(f\"Material distribution created in {(time.time() - start_time):.2f} seconds\\n\")\n",
    "print_distribution_info(face_dist, title=\"Simplified FABWAVE Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e84031-eb58-4320-bd51-35323dd566f1",
   "metadata": {},
   "source": [
    "# Machine Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e27a1",
   "metadata": {},
   "source": [
    "## ML-Ready Dataset Preparation\n",
    "\n",
    "The `DatasetLoader` prepares the merged dataset for machine learning training. It provides:\n",
    "\n",
    "- **Stratified Splitting**: Creates train/validation/test splits while preserving class distributions\n",
    "- **Subset Tracking**: Records file assignments in the dataset metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset for machine learning\n",
    "from hoops_ai.dataset import DatasetLoader\n",
    "\n",
    "flow_path = pathlib.Path(flow_file)\n",
    "loader = DatasetLoader(\n",
    "    merged_store_path=str(flow_path.parent / f\"{flow_path.stem}.dataset\"),  \n",
    "    parquet_file_path=str(flow_path.parent / f\"{flow_path.stem}.infoset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf88dc-7152-4209-8b8d-2e8f1e533584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hoops_ai.ml.EXPERIMENTAL import FlowTrainer\n",
    "\n",
    "\n",
    "flow_root_dir = nb_dir.joinpath(\"out\",\"flows\", flow_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom graph classification model\n",
    "import pathlib\n",
    "import sys\n",
    "notebooks_dir = pathlib.Path.cwd().parent\n",
    "sys.path.insert(0, str(notebooks_dir.parent))\n",
    "from custom_flow_model_graph_classification import CustomGraphClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9fb9a",
   "metadata": {},
   "source": [
    "## Import Custom Model\n",
    "\n",
    "Import the custom graph classification model that will be used for training all tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c0182",
   "metadata": {},
   "source": [
    "## Multi-Task Configuration\n",
    "\n",
    "The encoding configuration defines four tasks:\n",
    "\n",
    "**Task Configuration:**\n",
    "- **task_A**: 45 classes (0-44) - Original part categories\n",
    "- **task_B**: 45 classes (0-44) - Identical to task_A (for verification)\n",
    "- **task_C**: 5 groups (0-4) - Simplified categories  \n",
    "- **task_D**: 5 groups (0-4) - Identical to task_C (for verification)\n",
    "\n",
    "This multi-task setup allows training different classification tasks on the same dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf3a71",
   "metadata": {},
   "source": [
    "## Training Results Analysis\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "**Identical Results Within Pairs:**\n",
    "- **task_A â‰ˆ task_B**: Training/test metrics should be nearly identical (same 45-class problem)\n",
    "- **task_C â‰ˆ task_D**: Training/test metrics should be nearly identical (same 5-group problem)\n",
    "\n",
    "**Different Results Between Groups:**\n",
    "- **task_A/B vs task_C/D**: Metrics will differ (45 classes vs 5 groups are different problems)\n",
    "\n",
    "### How to Verify Training Results:\n",
    "\n",
    "View training logs in TensorBoard:\n",
    "```bash\n",
    "tensorboard --logdir out/flows/ETL_Multi_Y_Part_Classification/ml_output\n",
    "```\n",
    "\n",
    "Compare the following metrics:\n",
    "- **Loss curves**: Should be similar within pairs (Aâ‰ˆB, Câ‰ˆD)\n",
    "- **Accuracy**: Should be similar within pairs\n",
    "- **Confusion matrices**: Should show similar patterns within pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556861a",
   "metadata": {},
   "source": [
    "## Train task_A and task_B (45 classes)\n",
    "\n",
    "Training on the original 45-class labels. Both tasks use identical label encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset splits for task_A\n",
    "train_size, val_size, test_size = loader.split(\n",
    "    key=\"task_A\",\n",
    "    group=\"Labels\",\n",
    "    train=0.6, \n",
    "    validation=0.2, \n",
    "    test=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"task_A dataset split: Train={train_size}, Validation={val_size}, Test={test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train task_A (45 classes)\n",
    "part_classification_taskA = CustomGraphClassification(num_classes=45, result_dir=flow_root_dir)\n",
    "part_classification_taskA.set_label_for_training(\"task_A\")\n",
    "\n",
    "flow_trainer_taskA = FlowTrainer(\n",
    "    flowmodel       = part_classification_taskA,\n",
    "    datasetLoader   = loader,\n",
    "    experiment_name = \"HOOPS_AI_taskA_45classes\",\n",
    "    result_dir      = flow_root_dir,\n",
    "    accelerator     = 'gpu',\n",
    "    devices         = [0],\n",
    "    max_epochs      = 2,\n",
    "    batch_size      = 64\n",
    ")\n",
    "\n",
    "trained_model_taskA = flow_trainer_taskA.train()\n",
    "print(f\"\\\\nâœ… task_A training complete: {trained_model_taskA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset splits for task_B\n",
    "# Reset split state to ensure clean initialization\n",
    "loader.reset_split_state()\n",
    "train_size_b, val_size_b, test_size_b = loader.split(\n",
    "    key=\"task_B\",\n",
    "    group=\"Labels\",\n",
    "    train=0.7, \n",
    "    validation=0.15, \n",
    "    test=0.15, \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"task_B dataset split: Train={train_size_b}, Validation={val_size_b}, Test={test_size_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train task_B (45 classes - should be identical to task_A)\n",
    "part_classification_taskB = CustomGraphClassification(num_classes=45, result_dir=flow_root_dir)\n",
    "part_classification_taskB.set_label_for_training(\"task_B\")\n",
    "\n",
    "flow_trainer_taskB = FlowTrainer(\n",
    "    flowmodel       = part_classification_taskB,\n",
    "    datasetLoader   = loader,\n",
    "    experiment_name = \"HOOPS_AI_taskB_45classes\",\n",
    "    result_dir      = flow_root_dir,\n",
    "    accelerator     = 'gpu',\n",
    "    devices         = [0],\n",
    "    max_epochs      = 2,\n",
    "    batch_size      = 64\n",
    ")\n",
    "\n",
    "trained_model_taskB = flow_trainer_taskB.train()\n",
    "print(f\"\\\\nâœ… task_B training complete: {trained_model_taskB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c4a87",
   "metadata": {},
   "source": [
    "## Train task_C and task_D (5 groups)\n",
    "\n",
    "Training on the simplified 5-group labels. Both tasks use identical label encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and reinitialize loader for task_C and task_D\n",
    "loader.reset_split_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a09a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset splits for task_C\n",
    "train_size_c, val_size_c, test_size_c = loader.split(\n",
    "    key=\"task_C\",\n",
    "    group=\"Labels\",\n",
    "    train=0.7, \n",
    "    validation=0.15, \n",
    "    test=0.15, \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"task_C dataset split: Train={train_size_c}, Validation={val_size_c}, Test={test_size_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fca80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train task_C (5 groups)\n",
    "part_classification_taskC = CustomGraphClassification(num_classes=5, result_dir=flow_root_dir)\n",
    "part_classification_taskC.set_label_for_training(\"task_C\")\n",
    "\n",
    "flow_trainer_taskC = FlowTrainer(\n",
    "    flowmodel       = part_classification_taskC,\n",
    "    datasetLoader   = loader,\n",
    "    experiment_name = \"HOOPS_AI_taskC_5groups\",\n",
    "    result_dir      = flow_root_dir,\n",
    "    accelerator     = 'gpu',\n",
    "    devices         = [0],\n",
    "    max_epochs      = 2,\n",
    "    batch_size      = 64\n",
    ")\n",
    "\n",
    "trained_model_taskC = flow_trainer_taskC.train()\n",
    "print(f\"\\\\nâœ… task_C training complete: {trained_model_taskC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0483c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset splits for task_D\n",
    "loader.reset_split_state()\n",
    "train_size_d, val_size_d, test_size_d = loader.split(\n",
    "    key=\"task_D\",\n",
    "    group=\"Labels\",\n",
    "    train=0.8, \n",
    "    validation=0.1, \n",
    "    test=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"task_D dataset split: Train={train_size_d}, Validation={val_size_d}, Test={test_size_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd97954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train task_D (5 groups - should be identical to task_C)\n",
    "part_classification_taskD = CustomGraphClassification(num_classes=5, result_dir=flow_root_dir)\n",
    "part_classification_taskD.set_label_for_training(\"task_D\")\n",
    "\n",
    "flow_trainer_taskD = FlowTrainer(\n",
    "    flowmodel       = part_classification_taskD,\n",
    "    datasetLoader   = loader,\n",
    "    experiment_name = \"HOOPS_AI_taskD_5groups\",\n",
    "    result_dir      = flow_root_dir,\n",
    "    accelerator     = 'gpu',\n",
    "    devices         = [0],\n",
    "    max_epochs      = 2,\n",
    "    batch_size      = 64\n",
    ")\n",
    "\n",
    "trained_model_taskD = flow_trainer_taskD.train()\n",
    "print(f\"\\\\nâœ… task_D training complete: {trained_model_taskD}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HOOPS AI (CPU-DEV)",
   "language": "python",
   "name": "hoops_ai_cpu_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
